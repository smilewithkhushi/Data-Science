# -*- coding: utf-8 -*-
"""DSPROJECT_Smartphone Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c4X8hJv2sqre3RdJ9F_r7GxYISRAwIaN

**DATA SCIENCE PROJECT  : Analyze the dataset about smartphones features and then predict the price accordingly**.
**Submitted By - Khushi Panwar, Computer Science**

**1) Import data and libraries : import all useful libraries. Then, load the data.**
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

import sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

# load data (for exploring and cleaning we load test and train datasets)
data_train = pd.read_csv("train.csv")
data_test = pd.read_csv("test.csv")

"""**2) Fast looking on data : Let`s see head of our data frames, list of columns, sizes, descriptions and nan/null values in these datasets.**"""

# fast looking (size of dataframe, columns, 5 first rows of data, info and describing)
print(f"The train dataset has {data_train.shape[0]} rows.")
print(f"And {data_train.shape[1]} columns atleast")
print('-' * 50)
print(f"The test dataset has {data_test.shape[0]} rows.")
print(f"And {data_test.shape[1]} columns.")
print('-' * 50)
print(f"List of train dataset columns: {data_train.columns}")
print('-' * 50)
print(f"List of test dataset columns: {data_test.columns}")

print("-" * 50, data_train.info())
print('-' * 50, data_test.info())

"""**Information about columns:**
* id: ID
* battery_power: Total energy a battery can store in one time (mAh)
* clock_speed: Speed at which microprocessor executes instructions
* dual_sim: Support dual sim or not
* fc: Front Camera mega pixels
* four_g: Support 4G or not
* int_memory: Internal Memory (GB)
* m_dep: Mobile Depth (cm)
* mobile_wt: Weight of mobile phone
* n_cores: Number of cores of processor
* pc: Primary Camera mega pixels
* px_height: Pixel Resolution Height
* px_width: Pixel Resolution Width
* ram: Random Access Memory (MB)
* sc_h: Screen Height of mobile (cm)
* sc_w: Screen Width of mobile (cm)
* talk_time: Time that a single battery charge will last
* three_g: Support 3G or not
* touch_screen: Has touch screen or not
* wifi: Support wifi or not
"""

data_train.head()

data_train.describe()

"""**Null and NAN values.**"""

procent_of_null = data_train.isnull().sum() / data_train.shape[0]
print(procent_of_null)
print("-" * 20)
procent_of_nan = data_train.isna().sum() / data_train.shape[0]
print(procent_of_nan)

procent_of_null = data_test.isnull().sum() / data_test.shape[0]
print(procent_of_null)
print("-" * 20)
procent_of_nan = data_test.isna().sum() / data_test.shape[0]
print(procent_of_nan)

"""**How we can see train dataset includes 2000 rows and 20 columns, but test dataset includes 1000 rows and 20. We can look on names of columns and understand what they mean. Also, fortunately, we can see that there are not nan and null values.**

**3) Cleaning : Firstly, before modelling, we have to delete unnecessary columns to prevent overfitting, but to learn which columns we should drop we must create correlation matrix.**
"""

plt.figure(figsize=(20, 8))
correlation_rate = data_train.corr()
sns.heatmap(correlation_rate, annot = True, cmap = "Greens")

"""**Here we can see that there is strong positive correlation between price range and RAM. Also, price range, battery power and 4G/3G. Other features have small positive correlation.**

** 4) Exploring : We will explore columns of TRAIN dataset about RAM, 4G/3G and Battery Power, because these columns are important for our future modeling.**
"""

data_train.columns

selected=["battery_power", "ram" , "three_g", "price_range", "px_height" , "px_width", "fc"]

sns.pairplot(data_train[0:],hue='price_range', palette="Blues" )

sns.pairplot(data_train[selected],hue='price_range', palette="Blues" )

"""***4.1) RAM***"""

print(f"Max RAM value is: {data_train['ram'].max()} MB")
print(f"Min RAM value is: {data_train['ram'].min()} MB")
print(f"Mean of RAM values is: {round(data_train['ram'].mean())} MB")

plt.figure(figsize=[6, 5])
sns.boxplot(data_train['ram'], linewidth=2.5)
plt.xlabel("RAM value")

cheap = data_train["ram"][data_train["price_range"] == 1]
medium = data_train["ram"][data_train["price_range"] == 2]
expensive = data_train["ram"][data_train["price_range"] == 3]
price_ram_data = pd.DataFrame({"Low Price" : cheap,   "Medium Price" : medium,  "High Price" : expensive})

plt.figure(figsize=[6, 5])
sns.boxplot(data = price_ram_data, linewidth=2.5)
plt.ylabel("RAM")

"""**In these boxplots we can see:** 
**1)	In the first graph we can see maximum, minimum, median and mean of whole RAM column.**
**2)	In the second graph we can see comparison of price ranges and RAM amount in smartphones of these ranges.**

*4.2) Battery power*
"""

print(f"Max Battery Power value is: {data_train['battery_power'].max()} mAh")
print(f"Min Battery Power value is: {data_train['battery_power'].min()} mAh")
print(f"Mean of Battery Power values is: {round(data_train['battery_power'].mean())} mAh")

plt.figure(figsize=[6, 5])
sns.boxplot(data_train['battery_power'], linewidth=2.5)
plt.xlabel("Battery Power value")

cheap = data_train["battery_power"][data_train["price_range"] == 1]
medium = data_train["battery_power"][data_train["price_range"] == 2]
expensive = data_train["battery_power"][data_train["price_range"] == 3]
price_bp_data = pd.DataFrame({"Low Price" : cheap, "Medium Price" : medium, "High Price" : expensive})

plt.figure(figsize=[6, 5])
sns.boxplot(data = price_bp_data, linewidth=2.5)
plt.ylabel("Battery Power")

"""**In these boxplots we can see that:** **1)	In the first graph we can see maximum, minimum, median and mean of whole Battery Power column.**
**2)	In the second graph we can see comparison of price ranges and Battery Power amount in smartphones of these ranges. It is very interesting, because Battery Power values is almost similar in every price range.**

***4.3) 3G/4G***
"""

three_g = data_train["three_g"].value_counts().values
labels = ["Supported", "Not Supported"]

plt.figure(figsize=[6, 4])
plt.pie(three_g, labels = labels, shadow=True, startangle=90)
plt.title("Is 3G supported?")

dual_sim = data_train["dual_sim"].value_counts().values
plt.figure(figsize=[6, 4])
plt.pie(dual_sim, labels = labels, shadow=True, startangle=90)
plt.title("Is dual sim supported?")

four_g = data_train["four_g"].value_counts().values
labels = ["4G Supported", "Not Supported"]

plt.figure(figsize=[6, 4])
plt.pie(four_g, labels = labels, shadow=True, startangle=90)
plt.title("Is 4G supported?")

"""**REMOVING OUTLIERS**"""

''' Box plot use the IQR method to display data and outliers(shape of the data) but in order to be get a list of identified outlier, 
we will need to use the mathematical formula and retrieve the outlier data.'''
# First we will calculate IQR,
Q1 = data_train.quantile(0.25)
Q3 = data_train.quantile(0.75)
IQR = Q3 - Q1
print("IQR details for all columns : ", IQR)

'''  As we now have the IQR scores, it’s time to get hold on outliers. The below code will give an output with some true and false values. 
The data point where we have False that means these values are valid whereas True indicates presence of an outlier. '''
new_data= data_train[~((data_train < (Q1 - 1.5 * IQR)) |(data_train > (Q3 + 1.5 * IQR))).any(axis=1)]
print("Data after removing outliers : ", new_data)
new_data.shape

sns.pairplot(new_data[selected],hue='price_range', palette="Blues" )

"""**5) Modelling**

**Before modelling we have to prepare data. Let’s do this:**
"""

X = data_train.drop(["price_range"], axis = 1)
Y = data_train["price_range"]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 25)

new_data.columns

X = new_data.drop(["price_range"], axis = 1)
Y = new_data["price_range"]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 25)

"""**Now various models will be used. As our task is to classify price range (1,2,3) ; thats why creating classification models.**"""

X.info

"""*Logistic Regression*"""

log_reg = LogisticRegression()
log_reg.fit(X_train, Y_train)
print(f"Score is {log_reg.score(X_test, Y_test)}")

prediction1=log_reg.predict(X_test)
print(classification_report(Y_test, prediction1))

"""*Decision Tree*"""

tree = DecisionTreeClassifier(max_depth = 9)
tree.fit(X_train, Y_train)
print(f"Score is {tree.score(X_test, Y_test)}")

prediction2=tree.predict(X_test)
print(classification_report(Y_test, prediction2))

"""*KNN*"""

knn = KNeighborsClassifier(n_neighbors = 15)
knn.fit(X_train, Y_train)
print(f"Score is {knn.score(X_test, Y_test)}")

prediction3 = knn.predict(X_test)
print(classification_report(Y_test, prediction3))

"""*Random Forest Classifier*"""

forest = RandomForestClassifier(n_estimators = 300)
forest.fit(X_train, Y_train)
print(f"Score is {forest.score(X_test, Y_test)}")

prediction4 = forest.predict(X_test)
print(classification_report(Y_test, prediction4))

"""*SVM*"""

from sklearn.svm import SVC
supvec=SVC(kernel='rbf')
supvec.fit(X_train, Y_train)
prediction5=supvec.predict(X_test)


print(f"Score is {supvec.score(X_test, Y_test)}")
print(classification_report(Y_test, prediction5))

"""** Random Forest has given high accuracy/score..**

**6) Conclusion.**
"""

data_test.columns

data_train.columns

"""**A new dataset is created with our prediction values by Random Forest model**"""

print("DATA PREDICTION BY RANDOM FOREST ON TEST DATASET \n")
prediction_price = forest.predict(data_test.drop(["id"], axis = 1))
prepared_data = data_test
prepared_data["Predicted Price"] = prediction_price
prepared_data.head(5)